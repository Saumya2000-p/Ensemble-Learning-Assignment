{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "cbea887f-1b6f-4ca0-b96e-bc95eb4cd306",
      "cell_type": "code",
      "source": "                                                   ###Ensemble Learning Assignment###",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "65f39c56-1d67-4212-a68c-129f8e742b7f",
      "cell_type": "code",
      "source": "###Theoretical Queastions\n\n1. Can we use Bagging for regression problems?\n2. What is the difference between multiple model training and single model training?\n3. Explain the concept of feature randomness in Random Forest.\n4. What is OOB (Out-of-Bag) Score?\n5. How can you measure the importance of features in a Random Forest model?\n6. Explain the working principle of a Bagging Classifier.\n7. How do you evaluate a Bagging Classifier's performance?\n8. How does a Bagging Regressor work?\n9. What is the main advantage of ensemble techniques?\n10. What is the main challenge of ensemble methods?\n11. Explain the key idea behind ensemble techniques.\n12. What is a Random Forest Classifier?\n13. What are the main types of ensemble techniques?\n14. What is ensemble learning in machine learning?\n15. When should we avoid using ensemble methods?\n16. How does Bagging help in reducing overfitting?\n17. Why is Random Forest better than a single Decision Tree?\n18. What is the role of bootstrap sampling in Bagging?\n19. What are some real-world applications of ensemble techniques?\n20. What is the difference between Bagging and Boosting?\n\n###Practical Questions\n\n21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy.\n22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE).\n23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores.\n24. Train a Random Forest Regressor and compare its performance with a single Decision Tree.\n25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier.\n26. Train a Bagging Classifier using SVM as a base estimator and print accuracy.\n27. Train a Random Forest Classifier with different numbers of trees and compare accuracy.\n28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n29. Train a Random Forest Regressor and analyze feature importance scores.\n30. Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n32. Train a Bagging Regressor with different numbers of base estimators and compare performance.\n33. Train a Random Forest Classifier and analyze misclassified samples.\n34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier.\n35. Train a Random Forest Classifier and visualize the confusion matrix.\n36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy.\n37. Train a Random Forest Classifier and print the top 5 most important features.\n38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and Fl-score.\n39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy.\n40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance.\n41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score.\n42. Train a Bagging Classifier and evaluate its performance using cross-validation\n43. Train a Random Forest Classifier and plot the Precision-Recall curve\n44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy.\n45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "391eaae1-155b-4593-aa92-cc405645bfad",
      "cell_type": "code",
      "source": "Answer1:-Yes, Bagging can be used for regression problems.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5b082e93-f812-4b28-91ff-53f37f4b0f4f",
      "cell_type": "code",
      "source": "Answer2:-Multiple model training involves training multiple models on the same dataset, whereas single model training involves training a single model.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9831121d-d194-4f47-a0bf-2e919e6e4c56",
      "cell_type": "code",
      "source": "Answer3:-Feature randomness in Random Forest refers to the random selection of features at each node of a decision tree.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b7836501-d05a-47b4-a439-28dae72ffcee",
      "cell_type": "code",
      "source": "Answer4:-OOB Score is a measure of a model's performance on unseen data.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0dad1ca4-52f3-4ecc-a4ea-d698b256366f",
      "cell_type": "code",
      "source": "Answer5:-Feature importance can be measured using Gini importance or permutation importance.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5fa8dce4-8306-4415-b818-2258a1f27044",
      "cell_type": "code",
      "source": "Answer6:-A Bagging Classifier works by training multiple instances of a base classifier on different subsets of the training data.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7d9a2331-7ea9-4c72-80c3-a41d8f51f900",
      "cell_type": "code",
      "source": "Answer7:-A Bagging Classifier's performance can be evaluated using metrics like accuracy, precision, recall, and F1-score.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "71ffde9b-4d7f-46e1-a2b6-d86066924444",
      "cell_type": "code",
      "source": "Answer8:-A Bagging Regressor works similarly to a Bagging Classifier, but it combines the predictions of multiple regression models.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a98f2150-6970-4c60-8e80-ca9315162218",
      "cell_type": "code",
      "source": "Answer9:-The main advantage of ensemble techniques is that they can improve the accuracy and stability of predictions.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "db83f35b-b0b8-4943-a6f8-29c8005c0c53",
      "cell_type": "code",
      "source": "Answer10:-The main challenge of ensemble methods is that they can be computationally expensive and require careful tuning of hyperparameters.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6f15f569-d457-4b60-a09d-fc2f8f8e6a98",
      "cell_type": "code",
      "source": "Answer11:-The key idea behind ensemble techniques is to combine multiple models to improve performance.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3b9e570a-61df-4000-b0a8-e90e736e1b67",
      "cell_type": "code",
      "source": "Answer12:-A Random Forest Classifier is an ensemble learning method that combines multiple decision trees.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "04c2b287-0294-4895-8761-4cfef79f2f97",
      "cell_type": "code",
      "source": "Answer13:-The main types of ensemble techniques are Bagging, Boosting, and Stacking.\n",
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "82a3db9d-35d4-494a-aac7-ac448b7ebe44",
      "cell_type": "code",
      "source": "Answer14:-Ensemble learning is a technique that combines multiple models to improve performance.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e4a48bab-c487-4fcd-adfd-599bbd3eb3d2",
      "cell_type": "code",
      "source": "Answer15:-We should avoid using ensemble methods when the dataset is small or when the models are highly correlated.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a81a07f6-1df9-4f6f-9ff6-08ab5464de03",
      "cell_type": "code",
      "source": "Answer16:-Bagging helps in reducing overfitting by averaging the predictions of multiple models.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9f8e5108-58f6-4a52-92e2-88e67e3f5bbf",
      "cell_type": "code",
      "source": "Answer17:-Random Forest is better than a single Decision Tree because it reduces overfitting and improves accuracy.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "edfc7364-383a-450a-a392-5a4b4417390a",
      "cell_type": "code",
      "source": "Answer18:-Bootstrap sampling is used to create different subsets of the training data for each model.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c2ff68d4-2c02-468f-be82-cfa9c283ea39",
      "cell_type": "code",
      "source": "Answer19:-Ensemble techniques have been used in various applications, including image classification, speech recognition, and natural language processing.\n",
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d623bd6a-1ba1-404f-b270-0101e6ec465f",
      "cell_type": "code",
      "source": "Answer20:-Bagging and Boosting are both ensemble techniques, but Bagging combines multiple models in parallel, while Boosting combines multiple models in sequence.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "13d1cb78-0a0d-4b4f-b340-f2aabcca095d",
      "cell_type": "code",
      "source": "Answer21:-from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Bagging Classifier\nbagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\nbagging.fit(X_train, y_train)\n\n# Predict and evaluate accuracy\ny_pred = bagging.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Model Accuracy: {accuracy:.3f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2605ae6f-ea20-494b-90b0-9c030e4cfa47",
      "cell_type": "code",
      "source": "Answer22:-from sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Load dataset\ndiabetes = load_diabetes()\nX = diabetes.data\ny = diabetes.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Bagging Regressor\nbagging = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\nbagging.fit(X_train, y_train)\n\n# Predict and evaluate MSE\ny_pred = bagging.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"MSE: {mse:.3f}\")\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d5a5e331-3258-4615-a6ec-c09cd6a2efde",
      "cell_type": "code",
      "source": "Answer23:-from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\n\n# Load dataset\ncancer = load_breast_cancer()\nX = cancer.data\ny = cancer.target\n\n# Train Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X, y)\n\n# Print feature importance scores\nprint(\"Feature Importance Scores:\")\nfor i, feature in enumerate(cancer.feature_names):\n    print(f\"{feature}: {rf.feature_importances_[i]:.3f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "20310f16-ecdd-4a42-89de-ead3042f1d5a",
      "cell_type": "code",
      "source": "Answer24:-from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Load dataset\ndiabetes = load_diabetes()\nX = diabetes.data\ny = diabetes.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Train Decision Tree Regressor\ndt = DecisionTreeRegressor(random_state=42)\ndt.fit(X_train, y_train)\n\n# Predict and evaluate MSE\ny_pred_rf = rf.predict(X_test)\ny_pred_dt = dt.predict(X_test)\nmse_rf = mean_squared_error(y_test, y_pred_rf)\nmse_dt = mean_squared_error(y_test, y_pred_dt)\nprint(f\"Random Forest MSE: {mse_rf:.3f}\")\nprint(f\"Decision Tree MSE: {mse_dt:.3f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "19cb3975-8434-4a3e-94ed-9141edd2dabf",
      "cell_type": "code",
      "source": "Answer25:-from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\n\n# Load dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Train Random Forest Classifier with OOB score\nrf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\nrf.fit(X, y)\n\n# Print OOB score\nprint(f\"OOB Score: {rf.oob_score_:.3f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5c6233bb-4dbf-4f82-bd33-392d8854d2ad",
      "cell_type": "code",
      "source": "Answer26:-from sklearn.ensemble import BaggingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Bagging Classifier with SVM\nbagging = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=42)\nbagging.fit(X_train, y_train)\n\n# Predict and evaluate accuracy\ny_pred = bagging.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Model Accuracy: {accuracy:.3f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "37a22055-8146-4285-b469-5e059c81d145",
      "cell_type": "code",
      "source": "Answer27:-from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Random Forest Classifier with different numbers of trees\nn_estimators = [10, 50, 100, 200]\naccuracies = []\n\nfor n in n_estimators:\n    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n    rf.fit(X_train, y_train)\n    y_pred = rf.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    accuracies.append(accuracy)\n\n# Print accuracy for each number of trees\nfor n, accuracy in zip(n_estimators, accuracies):\n    print(f\"Number of Trees: {n}, Accuracy: {accuracy:.3f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "60ab4f14-e5aa-4f4e-a918-b9f99fe01646",
      "cell_type": "code",
      "source": "Answer28:-from sklearn.ensemble import BaggingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n# Load dataset\ncancer = load_breast_cancer()\nX = cancer.data\ny = cancer.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Bagging Classifier with Logistic Regression\nbagging = BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=10, random_state=42)\nbagging.fit(X_train, y_train)\n\n# Predict and evaluate AUC score\ny_pred_proba = bagging.predict_proba(X_test)[:, 1]\nauc = roc_auc_score(y_test, y_pred_proba)\nprint(f\"AUC Score: {auc:.3f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "70019bcd-0c13-43ca-982d-a2e77bfcaf37",
      "cell_type": "code",
      "source": "Answer29:-from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import load_diabetes\n\n# Load dataset\ndiabetes = load_diabetes()\nX = diabetes.data\ny = diabetes.target\n\n# Train Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X, y)\n\n# Print feature importance scores\nprint(\"Feature Importance Scores:\")\nfor i, feature in enumerate(diabetes.feature_names):\n    print(f\"{feature}: {rf.feature_importances_[i]:.3f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "439e29f8-ff2e-43dc-bae7-ad295d9e9d54",
      "cell_type": "code",
      "source": "Answer30:-from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Bagging Classifier\nbagging = BaggingClassifier(n_estimators=10, random_state=42)\nbagging.fit(X_train, y_train)\n\n# Train Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Predict and evaluate accuracy\ny_pred_bagging = bagging.predict(X_test)\ny_pred_rf = rf.predict(X_test)\naccuracy_bagging = accuracy_score(y_test, y_pred_bagging)\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\nprint(f\"Bagging Accuracy: {accuracy_bagging:.3f}\")\nprint(f\"Random Forest Accuracy: {accuracy_rf:.3f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "66b67787-8b67-4b10-ae82-62197fd45d9e",
      "cell_type": "code",
      "source": "Answer31:-from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.datasets import load_iris\n\n# Load dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Define hyperparameter grid\nparam_grid = {\n    'n_estimators': [10, 50, 100],\n    'max_depth': [None, 5, 10]\n}\n\n# Train Random Forest Classifier with GridSearchCV\nrf = RandomForestClassifier(random_state=42)\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5)\ngrid_search.fit(X, y)\n\n# Print best hyperparameters and score\nprint(f\"Best Hyperparameters: {grid_search.best_params_}\")\nprint(f\"Best Score: {grid_search.best_score_:.3f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ad2b5808-2659-490a-b2c1-b7381d1f3150",
      "cell_type": "code",
      "source": "Answer32:-from sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Load dataset\ndiabetes = load_diabetes()\nX = diabetes.data\ny = diabetes.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Bagging Regressor with different numbers of base estimators\nn_estimators = [10, 50, 100]\nmse_values = []\n\nfor n in n_estimators:\n    bagging = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=n, random_state=42)\n    bagging.fit(X_train, y_train)\n    y_pred = bagging.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n    mse_values.append(mse)\n\n# Print MSE values\nfor n, mse in zip(n_estimators, mse_values):\n    print(f\"Number of Base Estimators: {n}, MSE: {mse:.3f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fd8c27c1-44a5-4c9c-9756-8902806e7edb",
      "cell_type": "code",
      "source": "Answer33:-from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Load dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Predict and analyze misclassified samples\ny_pred = rf.predict(X_test)\nmisclassified_samples = X_test[y_test != y_pred]\nprint(f\"Number of Misclassified Samples: {len(misclassified_samples)}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8541ad8f-56ac-443a-906a-bcf9a98cdfb0",
      "cell_type": "code",
      "source": "Answer34:-from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Bagging Classifier\nbagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\nbagging.fit(X_train, y_train)\n\n# Train single Decision Tree Classifier\ndt = DecisionTreeClassifier(random_state=42)\ndt.fit(X_train, y_train)\n\n# Predict and evaluate accuracy\ny_pred_bagging = bagging.predict(X_test)\ny_pred_dt = dt.predict(X_test)\naccuracy_bagging = accuracy_score(y_test, y_pred_bagging)\naccuracy_dt = accuracy_score(y_test, y_pred_dt)\nprint(f\"Bagging Accuracy: {accuracy_bagging:.3f}\")\nprint(f\"Decision Tree Accuracy: {accuracy_dt:.3f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d5a10115-207d-4c35-8f61-290c8a3bb87b",
      "cell_type": "code",
      "source": "Answer35:-from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Predict and create confusion matrix\ny_pred = rf.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\n\n# Visualize confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, cmap=\"Blues\")\nplt.xlabel(\"Predicted labels\")\nplt.ylabel(\"True labels\")\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ff10e516-dc05-4c0f-8fa3-c313d3706cd3",
      "cell_type": "code",
      "source": "Answer36:-from sklearn.ensemble import StackingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define base estimators\nbase_estimators = [\n    ('dt', DecisionTreeClassifier(random_state=42)),\n    ('svm', SVC(probability=True, random_state=42)),\n    ('lr', LogisticRegression(random_state=42))\n]\n\n# Train Stacking Classifier\nstacking = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression(random_state=42))\nstacking.fit(X_train, y_train)\n\n# Predict and evaluate accuracy\ny_pred = stacking.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Stacking Accuracy: {accuracy:.3f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "382f1af2-d72c-40ca-be84-d3445fe9a71a",
      "cell_type": "code",
      "source": "Answer37:-from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\n\n# Load dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Train Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X, y)\n\n# Print top 5 most important features\nfeature_importances = rf.feature_importances_\ntop_5_features = sorted(zip(feature_importances, iris.feature_names), reverse=True)[:5]\nfor importance, feature in top_5_features:\n    print(f\"{feature}: {importance:.3f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e3489ff4-314f-42cc-b149-8f1b28be67f0",
      "cell_type": "code",
      "source": "Answer38:-from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Load dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Bagging Classifier\nbagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\nbagging.fit(X_train, y_train)\n\n# Predict and evaluate performance\ny_pred = bagging.predict(X_test)\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\nprint(f\"Precision: {precision:.3f}\")\nprint(f\"Recall: {recall:.3f}\")\nprint(f\"F1-score: {f1:.3f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "39c384c9-b84e-4753-b977-1ce28fd89e53",
      "cell_type": "code",
      "source": "Answer39:-from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\n# Load dataset\ndiabetes = load_diabetes()\nX = diabetes.data\ny = diabetes.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Predict and evaluate performance\ny_pred = rf.predict(X_test)\nmae = mean_absolute_error(y_test, y_pred)\nprint(f\"MAE: {mae:.3f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7fba6c18-8c5a-48dc-b7c8-00e2119cdb44",
      "cell_type": "code",
      "source": "Answer40:-from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Bagging Classifier with different base estimators\nbagging_dt = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\nbagging_svm = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=42)\n\nbagging_dt.fit(X_train, y_train)\nbagging_svm.fit(X_train, y_train)\n\n# Predict and evaluate performance\ny_pred_dt = bagging_dt.predict(X_test)\ny_pred_svm = bagging_svm.predict(X_test)\n\naccuracy_dt = accuracy_score(y_test, y_pred_dt)\naccuracy_svm = accuracy_score(y_test, y_pred_svm)\n\nprint(f\"Accuracy (Decision Tree): {accuracy_dt:.3f}\")\nprint(f\"Accuracy (SVM): {accuracy_svm:.3f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c79a536a-a127-44e3-a97e-3946e1c37293",
      "cell_type": "code",
      "source": "Answer41:-from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n# Load dataset\ncancer = load_breast_cancer()\nX = cancer.data\ny = cancer.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Predict and evaluate performance\ny_pred_proba = rf.predict_proba(X_test)[:, 1]\nauc = roc_auc_score(y_test, y_pred_proba)\nprint(f\"ROC-AUC Score: {auc:.3f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bb453d25-ae05-4e5d-82ad-2b23f925d5ea",
      "cell_type": "code",
      "source": "Answer42:-from sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\n# Load dataset\ndiabetes = load_diabetes()\nX = diabetes.data\ny = diabetes.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Bagging Regressor\nbagging = BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\nbagging.fit(X_train, y_train)\n\n# Predict and evaluate performance\ny_pred = bagging.predict(X_test)\nr2 = r2_score(y_test, y_pred)\nprint(f\"R-squared Score: {r2:.3f}\")\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "940343fe-5d34-41a5-bd92-2f48359258fe",
      "cell_type": "code",
      "source": "Answer43:-from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFE\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Load dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Random Forest Classifier with recursive feature elimination\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrfe = RFE(estimator=rf, n_features_to_select=2)\nrfe.fit(X_train, y_train)\n\n# Print selected features\nprint(f\"Selected Features: {rfe.support_}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4a53892f-3d0c-4344-ab93-1d7743ea1517",
      "cell_type": "code",
      "source": "Answer44:-from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import cross_val_score\n\n# Load dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Train Bagging Classifier\nbagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n\n# Evaluate performance using cross-validation\nscores = cross_val_score(bagging, X, y, cv=5)\nprint(f\"Cross-validation Scores: {scores}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1d669eca-7a86-41ed-b4f5-296e2d2e9358",
      "cell_type": "code",
      "source": "Answer45:-from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\n\n# Load dataset\ndiabetes = load_diabetes()\nX = diabetes.data\ny = diabetes.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Predict and evaluate performance\ny_pred = rf.predict(X_test)\nmsle = mean_squared_log_error(y_test, y_pred)\nprint(f\"MSLE: {msle:.3f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9e3e7776-13f7-42f4-ab6a-de05e4660771",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}