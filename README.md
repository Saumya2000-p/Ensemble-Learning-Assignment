# Ensemble-Learning-Assignment

Theoretical

1. Can we use Bagging for regression problems?

2. What is the difference between multiple model training and single model training?

3. Explain the concept of feature randomness in Random Forest.

4. What is OOB (Out-of-Bag) Score?

5. How can you measure the importance of features in a Random Forest model?

6. Explain the working principle of a Bagging Classifier.

7. How do you evaluate a Bagging Classifier's performance?

8. How does a Bagging Regressor work?

9. What is the main advantage of ensemble techniques?

10. What is the main challenge of ensemble methods?

11. Explain the key idea behind ensemble techniques.

12. What is a Random Forest Classifier?

13. What are the main types of ensemble techniques?

14. What is ensemble learning in machine learning?

15. When should we avoid using ensemble methods?

16. How does Bagging help in reducing overfitting?

17. Why is Random Forest better than a single Decision Tree?

18. What is the role of bootstrap sampling in Bagging?

19. What are some real-world applications of ensemble techniques?

20. What is the difference between Bagging and Boosting?

Practical

21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy.

22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE).

23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores.

24. Train a Random Forest Regressor and compare its performance with a single Decision Tree.

25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier.

26. Train a Bagging Classifier using SVM as a base estimator and print accuracy.

27. Train a Random Forest Classifier with different numbers of trees and compare accuracy.

28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.

29. Train a Random Forest Regressor and analyze feature importance scores.

30. Train an ensemble model using both Bagging and Random Forest and compare accuracy.

31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV

32. Train a Bagging Regressor with different numbers of base estimators and compare performance.

33. Train a Random Forest Classifier and analyze misclassified samples.

34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier.

35. Train a Random Forest Classifier and visualize the confusion matrix.

36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy.

37. Train a Random Forest Classifier and print the top 5 most important features.

38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and Fl-score.

39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy.

40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance.

41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score.

42. Train a Bagging Classifier and evaluate its performance using cross-validation

43. Train a Random Forest Classifier and plot the Precision-Recall curve

44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy.

45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance.
